{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4B: Regression\n",
    "## University of Toronto | Project by Riyan Roy\n",
    "\n",
    "In this part of the project, I extended linear regression analysis to explore additional features and regularization. I used a toy dataset (`LR_data.csv`) with 10 features (`x1`-`x10`) and a measurement `y`. The main focus was on linear regression, polynomial features, and regularization techniques.\n",
    "\n",
    "## Part 1: Data Preparation\n",
    "- Printed the dataset and created Numpy arrays with inputs (X) and outputs (y).\n",
    "- Split the dataset into training and validation sets (80% training, 20% validation).\n",
    "\n",
    "## Part 2: Linear Regression\n",
    "- Standardized the data using StandardScaler from sklearn.\n",
    "- Used `sklearn.linear_model.LinearRegression` to perform linear regression.\n",
    "- Printed the RMSE for training and validation data.\n",
    "\n",
    "## Part 3: Linear Regression with Additional Features\n",
    "- Added more features to our dataset (degree 8) using `sklearn.preprocessing.PolynomialFeatures`.\n",
    "- Performed linear regression using `sklearn.linear_model.LinearRegression`.\n",
    "- Printed the RMSE for training and validation data.\n",
    "\n",
    "## Part 4: Linear Regression with Additional Features and Regularization\n",
    "- Used `sklearn.linear_model.Ridge` to perform linear regression with regularization.\n",
    "- Applied the model to the processed data from Part 3.\n",
    "- Swept `alpha` from 1E-2 to 1E10 to find the optimal regularization strength.\n",
    "\n",
    "## Findings and Results\n",
    "- Linear Regression (Part 2) provided baseline performance.\n",
    "- Addition of Polynomial Features (Part 3) improved the model.\n",
    "- Regularization (Part 4) helped control overfitting.\n",
    "- The choice of `alpha` in regularization influenced model performance.\n",
    "- The project provided insights into the impact of additional features and regularization on linear regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sy0M4UZZNEk2"
   },
   "source": [
    "## Gradient Descent with Additional Features and Regularization [2 marks + 1 mark Git submission]\n",
    "\n",
    "We'll apply linear regresssion to a toy dataset (`LR_data.csv`), with 10 features `x1`-`x10` and a \"measurement\" `y`. We'll take a few shortcuts by using built-in sklearn functions.\n",
    "\n",
    "1. Data Preparation **[0.5]**\n",
    "  * Print the dataset, and create Numpy arrays with inputs (X) and outputs (y). \n",
    "  * Split the dataset into training and validation sets (80% training, 20% validation). When splitting, set `random_state=1`.\n",
    "\n",
    "2. Linear Regression **[0.5]**\n",
    "  * Standardize the data using StandardScaler from sklearn.\n",
    "  * Use the `sklearn.linear_model.LinearRegression` function [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) to perform linear regression.\n",
    "  * Print the RMSE for training and validation data.\n",
    "\n",
    "3. Linear Regression with Additional Features **[0.5]**\n",
    "  * Let's add more features to our dataset (degree 8) using `sklearn.preprocessing.PolynomialFeatures` [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html). You'll want to create the additional features first, then perform standardization (start from non-standardized data).\n",
    "  * Again, use `sklearn.linear_model.LinearRegression` to perform linear regression.\n",
    "  * Print the RMSE for training and validation data.\n",
    "\n",
    "4. Linear Regression with Additional Features and Regularization **[0.5]**\n",
    "  * Let's switch models, and instead use the `sklearn.linear_model.Ridge` function [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) to perform linear regression with regularization. Apply the model to the processed data (additional, standardized) you used in 3 above. Use a `FOR` loop to run `sklearn.linear_model.Ridge` with different `alpha` values. Specifically, sweep `alpha` from 1E-2 to 1E10 (each step is an order of magnitude jump)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6ZKR243TOERR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split  \n",
    "df=pd.read_csv(\"https://raw.githubusercontent.com/aps1070-2019/datasets/master/LR_data.csv\" , skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MCHTbh05YW3O"
   },
   "outputs": [],
   "source": [
    "x_array = df.iloc[:,0:-1]\n",
    "y_array = df.iloc[:,-1]\n",
    "x_array=np.array(x_array)\n",
    "y_array = np.array(y_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cB63sEgaYfl_"
   },
   "outputs": [],
   "source": [
    "x_train,x_val,y_train,y_val = train_test_split(x_array,y_array,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CVqT89MbZ7Jt"
   },
   "outputs": [],
   "source": [
    "x = pd.DataFrame(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lEgU2EJpaUHo"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "std = scaler.fit(x_train)\n",
    "std_x_train = std.transform(x_train)\n",
    "std_x_val = std.transform(x_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "a6rV__qxaogG"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "linear_reg = LinearRegression().fit(std_x_train, y_train)\n",
    "y_pred_train = linear_reg.predict(std_x_train)\n",
    "y_pred_val = linear_reg.predict(std_x_val)\n",
    "\n",
    "rmse_train = np.sqrt(mean_squared_error(y_pred_train,y_train))\n",
    "rmse_val = np.sqrt(mean_squared_error(y_pred_val,y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FxsFQ_3NC6l8",
    "outputId": "540813d3-530a-4a36-aef6-68e0a465af34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE (Using simple linear regression) for training set is 16296980.655667374 & for validation set is 14061578.864980102\n"
     ]
    }
   ],
   "source": [
    "print('The RMSE (Using simple linear regression) for training set is {} & for validation set is {}'.format(rmse_train,rmse_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PhZdsTuTedf6"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(8)\n",
    "poly_x_train = poly.fit_transform(x_train)\n",
    "poly_x_val = poly.transform(x_val)\n",
    "\n",
    "scaler1 = StandardScaler()\n",
    "std1 = scaler1.fit(poly_x_train)\n",
    "std_poly_x_train = std1.transform(poly_x_train)\n",
    "std_poly_x_val = std1.transform(poly_x_val)\n",
    "\n",
    "linear_reg1 = LinearRegression().fit(std_poly_x_train, y_train)\n",
    "poly_y_pred_train = linear_reg1.predict(std_poly_x_train)\n",
    "poly_y_pred_val = linear_reg1.predict(std_poly_x_val)\n",
    "\n",
    "poly_rmse_train = np.sqrt(mean_squared_error(poly_y_pred_train,y_train))\n",
    "poly_rmse_val = np.sqrt(mean_squared_error(poly_y_pred_val,y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ozVjbKT0F9n5",
    "outputId": "deba9394-e914-474c-8dbe-f5e2100534dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE (Using feature mapping on linear regression) for training set is 1.1447930035368027e-07 & for validation set is 10920843.908991363\n"
     ]
    }
   ],
   "source": [
    "print('The RMSE (Using feature mapping on linear regression) for training set is {} & for validation set is {}'.format(poly_rmse_train,poly_rmse_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ha2rk69FpYN2"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "r_rmse_train = []\n",
    "r_rmse_val = []\n",
    "i = 0.01\n",
    "for i in [0.01,0.1,1,10,100,1000,10000,100000,1000000,10000000,100000000,100000000,10000000000]:\n",
    "  ridge = Ridge(alpha = i)\n",
    "  ridge_reg = ridge.fit(std_poly_x_train,y_train)\n",
    "  ridge_y_pred_train = ridge_reg.predict(std_poly_x_train)\n",
    "  ridge_y_pred_val = ridge_reg.predict(std_poly_x_val)\n",
    "  \n",
    "  ridge_rmse_train = np.sqrt(mean_squared_error(ridge_y_pred_train,y_train))\n",
    "  ridge_rmse_val = np.sqrt(mean_squared_error(ridge_y_pred_val,y_val))\n",
    "\n",
    "  r_rmse_train.append(ridge_rmse_train)\n",
    "  r_rmse_val.append(ridge_rmse_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xBl3soi1GGkc",
    "outputId": "eb0d3b3c-c938-4164-a309-f742a39dfc42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE (Using feature mapping and ridge regression) for training set is 23499904.341360077 & for validation set is 19236486.536391094\n"
     ]
    }
   ],
   "source": [
    "print('The RMSE (Using feature mapping and ridge regression) for training set is {} & for validation set is {}'.format(r_rmse_train[-1],r_rmse_val[-1]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
